{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/users/rohib/miniconda3/envs/signal/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import logging\n",
    "import os\n",
    "import random\n",
    "import sys\n",
    "import pdb\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "#from fedml_api.model.cv.lenet5 import LeNet5\n",
    "# sys.path.append('/data/users2/bthapaliya/DistributedFLExperiments/DistributedFL')\n",
    "# sys.path.append('/data/users2/bthapaliya/DistributedFLExperiments/DistributedFL/fedml_api')\n",
    "\n",
    "sys.path.insert(0, os.path.abspath(\"/data/users2/bthapaliya/DistributedFLExperiments/DistributedFL/data/\"))\n",
    "from fedml_api.model.cv.salient_models import AlexNet3D_Dropout, ResNet_l3\n",
    "\n",
    "from fedml_api.data_preprocessing.cifar100.data_loader import load_partition_data_cifar100\n",
    "from fedml_api.model.cv.vgg import vgg16, vgg11\n",
    "from fedml_api.model.cv.cnn_cifar10 import cnn_cifar10, cnn_cifar100\n",
    "from fedml_api.standalone.DisPFL.dispfl_api import dispflAPI\n",
    "from fedml_api.standalone.sailentgrads.sailentgrads_api import SailentGradsAPI\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fedml_api.data_preprocessing.cifar10.data_loader import load_partition_data_cifar10\n",
    "from fedml_api.data_preprocessing.ABCD.data_loader import load_partition_data_abcd\n",
    "from fedml_api.data_preprocessing.tiny_imagenet.data_loader import load_partition_data_tiny\n",
    "from fedml_api.model.cv.resnet import  customized_resnet18, original_resnet18, tiny_resnet18\n",
    "from fedml_api.standalone.sailentgrads.my_model_trainer import MyModelTrainer\n",
    "\n",
    "from fedml_api.standalone.sailentgrads.client import Client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Args:\n",
    "    model ='3DCNN'\n",
    "    dataset = 'ABCD'\n",
    "    data_dir ='/data/users2/bthapaliya/NeuroimageDistributedFL/DistributedFL'\n",
    "    partition_method='dir'\n",
    "    partition_alpha=0.3\n",
    "    batch_size=16\n",
    "    client_optimizer='sgd'\n",
    "    lr=0.001\n",
    "    lr_decay=0.998\n",
    "    wd=5e-4\n",
    "    epochs=2\n",
    "    client_num_in_total = 6\n",
    "    frac = 0.5\n",
    "    momentum=0\n",
    "    comm_round=200\n",
    "    frequency_of_the_test=1\n",
    "    gpu=0\n",
    "    ci=0\n",
    "    dense_ratio=0.5\n",
    "    anneal_factor=0.5\n",
    "    seed=1024\n",
    "    cs='v0'\n",
    "    itersnip_iteration = 1\n",
    "    stratified_sampling ='store_true'\n",
    "    active=1.0\n",
    "    public_portion=0\n",
    "    erk_power_scale=1\n",
    "    dis_gradient_check=False\n",
    "    strict_avg= False\n",
    "    static = False\n",
    "    uniform = False\n",
    "    save_masks = False\n",
    "    different_initial = False\n",
    "    record_mask_diff = False\n",
    "    diff_spa = False\n",
    "    global_test = False\n",
    "    tag=\"test\"\n",
    "    snip_mask=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# python main_sailentgrads.py --model 'resnet18' \\\n",
    "# --dataset 'cifar10' \\\n",
    "# --partition_method 'dir' \\\n",
    "# --partition_alpha 0.3 \\\n",
    "# --batch_size 16 \\\n",
    "# --lr 0.1 \\\n",
    "# --lr_decay 0.998 \\\n",
    "# --epochs 5 \\\n",
    "# --dense_ratio 0.1 \\\n",
    "# --client_num_in_total 100 --frac 0.1 \\\n",
    "# --comm_round 500 \\\n",
    "# --seed 2022"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = Args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(args, model_name,class_num):\n",
    "    model = None\n",
    "    if model_name == \"3DCNN\":\n",
    "        model = AlexNet3D_Dropout(num_classes=class_num)\n",
    "    if model_name == \"cnn_cifar10\":\n",
    "        model = cnn_cifar10()\n",
    "    elif model_name == \"cnn_cifar100\":\n",
    "        model = cnn_cifar100()\n",
    "    elif model_name == \"resnet18\" and args.dataset != 'tiny':\n",
    "        model = customized_resnet18(class_num=class_num)\n",
    "    elif model_name == \"resnet18\" and args.dataset == 'tiny':\n",
    "        model = tiny_resnet18(class_num=class_num)\n",
    "    elif model_name == \"vgg11\":\n",
    "        model = vgg11(class_num)\n",
    "    return model\n",
    "\n",
    "\n",
    "def custom_model_trainer(args, model, logger):\n",
    "    return MyModelTrainer(model, args, logger)\n",
    "\n",
    "def logger_config(log_path, logging_name):\n",
    "    logger = logging.getLogger(logging_name)\n",
    "    logger.setLevel(level=logging.DEBUG)\n",
    "    handler = logging.FileHandler(log_path, mode='w+',encoding='UTF-8')\n",
    "    handler.setLevel(level=logging.DEBUG)\n",
    "    formatter = logging.Formatter('%(message)s')\n",
    "    handler.setFormatter(formatter)\n",
    "    logger.addHandler(handler)\n",
    "    return logger\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(args, dataset_name):\n",
    "    if dataset_name == \"ABCD\":\n",
    "        args.data_dir += \"ABCD\"\n",
    "        train_data_num, test_data_num, train_data_global, test_data_global, \\\n",
    "        train_data_local_num_dict, train_data_local_dict, test_data_local_dict, \\\n",
    "        class_num = load_partition_data_abcd(args.data_dir, args.partition_method,\n",
    "                                args.partition_alpha, args.client_num_in_total, args.batch_size, logger)\n",
    "\n",
    "    if dataset_name == \"cifar10\":\n",
    "        args.data_dir += \"cifar10\"\n",
    "        train_data_num, test_data_num, train_data_global, test_data_global, \\\n",
    "        train_data_local_num_dict, train_data_local_dict, test_data_local_dict, \\\n",
    "        class_num = load_partition_data_cifar10(args.data_dir, args.partition_method,\n",
    "                                args.partition_alpha, args.client_num_in_total, args.batch_size, logger)\n",
    "    elif dataset_name == \"cifar100\":\n",
    "        args.data_dir += \"cifar100\"\n",
    "        train_data_num, test_data_num, train_data_global, test_data_global, \\\n",
    "        train_data_local_num_dict, train_data_local_dict, test_data_local_dict, \\\n",
    "        class_num = load_partition_data_cifar100(args.data_dir, args.partition_method,\n",
    "                                                args.partition_alpha, args.client_num_in_total, args.batch_size, logger)\n",
    "    elif dataset_name == \"tiny\":\n",
    "        args.data_dir += \"tiny_imagenet\"\n",
    "        train_data_num, test_data_num, train_data_global, test_data_global, \\\n",
    "        train_data_local_num_dict, train_data_local_dict, test_data_local_dict, \\\n",
    "        class_num = load_partition_data_tiny(args.data_dir, args.partition_method,\n",
    "                                             args.partition_alpha, args.client_num_in_total,\n",
    "                                                 args.batch_size, logger)\n",
    "\n",
    "    dataset = [train_data_num, test_data_num, train_data_global, test_data_global,\n",
    "               train_data_local_num_dict, train_data_local_dict, test_data_local_dict, class_num]\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch version1.12.1+cu116\n"
     ]
    }
   ],
   "source": [
    "print(\"torch version{}\".format(torch.__version__))\n",
    "device = torch.device(\"cuda:\" + str(args.gpu) if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "data_partition=args.partition_method\n",
    "if data_partition!=\"homo\":\n",
    "    data_partition+=str(args.partition_alpha)\n",
    "args.identity = \"SailentGrads\" + \"-\" + args.dataset + \"-\" + data_partition\n",
    "args.identity+=\"-mdl\" + args.model + \"customized\" +\"lowbatch\"\n",
    "args.identity+=\"-cs\"+args.cs\n",
    "\n",
    "args.identity += \"-cm\" + str(args.comm_round) + \"-total_clnt\" + str(args.client_num_in_total)\n",
    "args. client_num_per_round = int(args.client_num_in_total* args.frac)\n",
    "args.identity += \"-neighbor\" + str(args.client_num_per_round)\n",
    "args.identity += \"-dr\" + str(args.dense_ratio)\n",
    "args.identity += \"-active\" + str(args.active)\n",
    "args.identity += '-seed' + str(args.seed)\n",
    "args.identity += '-batchsize' + str(args.batch_size)\n",
    "args.identity += '-iteration' + str(args.itersnip_iteration)\n",
    "args.identity += '-stratified' + str(args.stratified_sampling)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_path = os.path.join('./', 'LOG/test' + '.log')\n",
    "main_log_path = os.path.join('LOG/' + args.dataset)\n",
    "if not os.path.exists(main_log_path):\n",
    "    os.makedirs(main_log_path)\n",
    "logger = logger_config(log_path='LOG/' + args.dataset + '.log', logging_name=args.identity)\n",
    "\n",
    "logger.info(args)\n",
    "logger.info(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset = load_data(args, \"ABCD\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('abcd.pickle', 'wb') as handle:\n",
    "#     pickle.dump(dataset, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('abcd.pickle', 'rb') as handle:\n",
    "    dataset = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create model.\n",
    "model = create_model(args, model_name=args.model,class_num=1)\n",
    "model = model.to(device)\n",
    "model_trainer = custom_model_trainer(args, model, logger)\n",
    "logger.info(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_clients(train_data_local_num_dict, train_data_local_dict, test_data_local_dict, model_trainer):\n",
    "    for client_idx in range(args.client_num_in_total):\n",
    "        c = Client(client_idx, train_data_local_dict[client_idx], test_data_local_dict[client_idx],\n",
    "                    train_data_local_num_dict[client_idx], args, device, model_trainer, logger)\n",
    "        client_list.append(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "[train_data_num, test_data_num, train_data_global, test_data_global,\n",
    "  train_data_local_num_dict, train_data_local_dict, test_data_local_dict, class_counts] = dataset\n",
    "\n",
    "client_list = []\n",
    "setup_clients(train_data_local_num_dict, train_data_local_dict, test_data_local_dict, model_trainer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get client model and client data\n",
    "clnt = client_list[1]\n",
    "clnt_data = clnt.local_training_data\n",
    "clnt_testdata = clnt.local_test_data\n",
    "model = clnt.model_trainer.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "x,labels,z = next(iter(clnt_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = x.to(device)  # Convert to tensor\n",
    "x = x.unsqueeze(1)\n",
    "\n",
    "x, labels = x.to(device), labels.to(device)\n",
    "model.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "args.epochs=40"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.SGD(filter(lambda p: p.requires_grad, model.parameters()), lr=0.01, momentum=args.momentum,weight_decay=args.wd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, optimizer, train_data,  device,  args, round, epochs):\n",
    "    # torch.manual_seed(0)\n",
    "    model = model\n",
    "    model.to(device)\n",
    "    model.train()\n",
    "    # train and update\n",
    "    criterion = nn.BCEWithLogitsLoss().to(device)\n",
    "    for epoch in range(epochs):\n",
    "        epoch_loss = []\n",
    "        #for batch_idx, (x, labels) in enumerate(train_data):\n",
    "        for x, labels, _ in train_data:\n",
    "            #For 3DConv Network\n",
    "            #x = torch.tensor(x, dtype=torch.float32)  # Convert to tensor\n",
    "            x = x.to(device)  # Convert to tensor\n",
    "            x = x.unsqueeze(1)\n",
    "\n",
    "            x, labels = x.to(device), labels.to(device)\n",
    "            model.zero_grad()\n",
    "            log_probs = model.forward(x)\n",
    "            loss = criterion(log_probs, labels.unsqueeze(1).float())\n",
    "            loss.backward()\n",
    "            # to avoid nan loss\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 10)\n",
    "            optimizer.step()\n",
    "            epoch_loss.append(loss.item())\n",
    "            # for name, param in model.named_parameters():\n",
    "            #     if name in masks:\n",
    "            #         param.data *= masks[name].to(device)\n",
    "        print('Epoch: {}\\tLoss: {:.6f}\\t LR: {:.4f}'.format(epoch, sum(epoch_loss) / len(epoch_loss), optimizer.param_groups[0]['lr']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0\tLoss: 0.828396\t LR: 0.0100\n",
      "Epoch: 1\tLoss: 0.645710\t LR: 0.0100\n",
      "Epoch: 2\tLoss: 0.506960\t LR: 0.0100\n",
      "Epoch: 3\tLoss: 0.911434\t LR: 0.0100\n",
      "Epoch: 4\tLoss: 0.714987\t LR: 0.0100\n",
      "Epoch: 5\tLoss: 0.841519\t LR: 0.0100\n",
      "Epoch: 6\tLoss: 0.645194\t LR: 0.0100\n",
      "Epoch: 7\tLoss: 0.714814\t LR: 0.0100\n",
      "Epoch: 8\tLoss: 0.612401\t LR: 0.0100\n",
      "Epoch: 9\tLoss: 0.623211\t LR: 0.0100\n",
      "Epoch: 10\tLoss: 0.759901\t LR: 0.0100\n",
      "Epoch: 11\tLoss: 0.676069\t LR: 0.0100\n",
      "Epoch: 12\tLoss: 0.605578\t LR: 0.0100\n",
      "Epoch: 13\tLoss: 0.586054\t LR: 0.0100\n",
      "Epoch: 14\tLoss: 0.625682\t LR: 0.0100\n",
      "Epoch: 15\tLoss: 0.524836\t LR: 0.0100\n",
      "Epoch: 16\tLoss: 0.664885\t LR: 0.0100\n",
      "Epoch: 17\tLoss: 0.815500\t LR: 0.0100\n",
      "Epoch: 18\tLoss: 0.539739\t LR: 0.0100\n",
      "Epoch: 19\tLoss: 0.593757\t LR: 0.0100\n",
      "Epoch: 20\tLoss: 0.740088\t LR: 0.0100\n",
      "Epoch: 21\tLoss: 0.673250\t LR: 0.0100\n",
      "Epoch: 22\tLoss: 0.556431\t LR: 0.0100\n",
      "Epoch: 23\tLoss: 0.505106\t LR: 0.0100\n",
      "Epoch: 24\tLoss: 0.545667\t LR: 0.0100\n",
      "Epoch: 25\tLoss: 0.669968\t LR: 0.0100\n",
      "Epoch: 26\tLoss: 0.453069\t LR: 0.0100\n",
      "Epoch: 27\tLoss: 0.971069\t LR: 0.0100\n",
      "Epoch: 28\tLoss: 0.518837\t LR: 0.0100\n",
      "Epoch: 29\tLoss: 0.514169\t LR: 0.0100\n",
      "Epoch: 30\tLoss: 0.429980\t LR: 0.0100\n",
      "Epoch: 31\tLoss: 0.940039\t LR: 0.0100\n",
      "Epoch: 32\tLoss: 0.701802\t LR: 0.0100\n",
      "Epoch: 33\tLoss: 0.411875\t LR: 0.0100\n",
      "Epoch: 34\tLoss: 0.552157\t LR: 0.0100\n",
      "Epoch: 35\tLoss: 0.383228\t LR: 0.0100\n",
      "Epoch: 36\tLoss: 0.379529\t LR: 0.0100\n",
      "Epoch: 37\tLoss: 0.367168\t LR: 0.0100\n",
      "Epoch: 38\tLoss: 0.310366\t LR: 0.0100\n",
      "Epoch: 39\tLoss: 0.290155\t LR: 0.0100\n",
      "Epoch: 40\tLoss: 0.375874\t LR: 0.0100\n",
      "Epoch: 41\tLoss: 0.294348\t LR: 0.0100\n",
      "Epoch: 42\tLoss: 0.460386\t LR: 0.0100\n",
      "Epoch: 43\tLoss: 0.503014\t LR: 0.0100\n",
      "Epoch: 44\tLoss: 0.442807\t LR: 0.0100\n",
      "Epoch: 45\tLoss: 0.711437\t LR: 0.0100\n",
      "Epoch: 46\tLoss: 0.777996\t LR: 0.0100\n",
      "Epoch: 47\tLoss: 0.486554\t LR: 0.0100\n",
      "Epoch: 48\tLoss: 0.418692\t LR: 0.0100\n",
      "Epoch: 49\tLoss: 0.394539\t LR: 0.0100\n",
      "Epoch: 50\tLoss: 0.345730\t LR: 0.0100\n",
      "Epoch: 51\tLoss: 0.553989\t LR: 0.0100\n",
      "Epoch: 52\tLoss: 0.562422\t LR: 0.0100\n",
      "Epoch: 53\tLoss: 0.225350\t LR: 0.0100\n",
      "Epoch: 54\tLoss: 0.430084\t LR: 0.0100\n",
      "Epoch: 55\tLoss: 0.336384\t LR: 0.0100\n",
      "Epoch: 56\tLoss: 0.300461\t LR: 0.0100\n",
      "Epoch: 57\tLoss: 0.245106\t LR: 0.0100\n",
      "Epoch: 58\tLoss: 0.342627\t LR: 0.0100\n",
      "Epoch: 59\tLoss: 0.116502\t LR: 0.0100\n",
      "Epoch: 60\tLoss: 0.185211\t LR: 0.0100\n",
      "Epoch: 61\tLoss: 0.227043\t LR: 0.0100\n",
      "Epoch: 62\tLoss: 0.173417\t LR: 0.0100\n",
      "Epoch: 63\tLoss: 0.074653\t LR: 0.0100\n",
      "Epoch: 64\tLoss: 0.032529\t LR: 0.0100\n",
      "Epoch: 65\tLoss: 0.031416\t LR: 0.0100\n",
      "Epoch: 66\tLoss: 0.186263\t LR: 0.0100\n",
      "Epoch: 67\tLoss: 0.595308\t LR: 0.0100\n",
      "Epoch: 68\tLoss: 0.057426\t LR: 0.0100\n",
      "Epoch: 69\tLoss: 0.022445\t LR: 0.0100\n",
      "Epoch: 70\tLoss: 0.014233\t LR: 0.0100\n",
      "Epoch: 71\tLoss: 0.011286\t LR: 0.0100\n",
      "Epoch: 72\tLoss: 0.020359\t LR: 0.0100\n",
      "Epoch: 73\tLoss: 0.041840\t LR: 0.0100\n",
      "Epoch: 74\tLoss: 0.337506\t LR: 0.0100\n",
      "Epoch: 75\tLoss: 0.020428\t LR: 0.0100\n",
      "Epoch: 76\tLoss: 0.031673\t LR: 0.0100\n",
      "Epoch: 77\tLoss: 0.024285\t LR: 0.0100\n",
      "Epoch: 78\tLoss: 0.203468\t LR: 0.0100\n",
      "Epoch: 79\tLoss: 0.044678\t LR: 0.0100\n",
      "Epoch: 80\tLoss: 0.034084\t LR: 0.0100\n",
      "Epoch: 81\tLoss: 0.006777\t LR: 0.0100\n",
      "Epoch: 82\tLoss: 0.009745\t LR: 0.0100\n",
      "Epoch: 83\tLoss: 0.013004\t LR: 0.0100\n",
      "Epoch: 84\tLoss: 0.008260\t LR: 0.0100\n",
      "Epoch: 85\tLoss: 0.002413\t LR: 0.0100\n",
      "Epoch: 86\tLoss: 0.009870\t LR: 0.0100\n",
      "Epoch: 87\tLoss: 0.022925\t LR: 0.0100\n",
      "Epoch: 88\tLoss: 0.156966\t LR: 0.0100\n",
      "Epoch: 89\tLoss: 0.019416\t LR: 0.0100\n",
      "Epoch: 90\tLoss: 0.007186\t LR: 0.0100\n",
      "Epoch: 91\tLoss: 0.008564\t LR: 0.0100\n",
      "Epoch: 92\tLoss: 0.066462\t LR: 0.0100\n",
      "Epoch: 93\tLoss: 0.030983\t LR: 0.0100\n",
      "Epoch: 94\tLoss: 0.003352\t LR: 0.0100\n",
      "Epoch: 95\tLoss: 0.005541\t LR: 0.0100\n",
      "Epoch: 96\tLoss: 0.002475\t LR: 0.0100\n",
      "Epoch: 97\tLoss: 0.003314\t LR: 0.0100\n",
      "Epoch: 98\tLoss: 0.014311\t LR: 0.0100\n",
      "Epoch: 99\tLoss: 0.009381\t LR: 0.0100\n"
     ]
    }
   ],
   "source": [
    "train(model, optimizer, clnt_data, device, args, round=0, epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.001"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimizer.param_groups[0]['lr'] = 0.001\n",
    "optimizer.param_groups[0]['lr']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, test_data, device):\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    metrics = {\n",
    "        'test_correct': 0,\n",
    "        'test_acc':0.0,\n",
    "        'test_loss': 0,\n",
    "        'test_total': 0\n",
    "    }\n",
    "\n",
    "    criterion = nn.BCEWithLogitsLoss().to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        #for batch_idx, (x, target) in enumerate(test_data):\n",
    "        for x, target, _ in test_data:\n",
    "            #For 3DConv Network\n",
    "            #x = torch.tensor(x, dtype=torch.float32)  # Convert to tensor\n",
    "            x = x.to(device)  # Convert to tensor\n",
    "            x = x.unsqueeze(1)\n",
    "\n",
    "            #x = x.to(device)\n",
    "            target = target.to(device)\n",
    "            pred = model(x)\n",
    "            loss = criterion(pred, target.unsqueeze(1).float())\n",
    "\n",
    "            _, predicted = torch.max(pred, -1)\n",
    "            correct = predicted.eq(target).sum()\n",
    "\n",
    "            metrics['test_correct'] += correct.item()\n",
    "            metrics['test_loss'] += loss.item() * target.size(0)\n",
    "            metrics['test_total'] += target.size(0)\n",
    "            metrics['test_acc'] = metrics['test_correct'] / metrics['test_total']\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'test_correct': 5,\n",
       " 'test_acc': 0.625,\n",
       " 'test_loss': 19.62701416015625,\n",
       " 'test_total': 8}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test(model, clnt_testdata, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "for i, data in enumerate(clnt_testdata):\n",
    "    print(i)\n",
    "    data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 121, 145, 121])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<fedml_api.standalone.sailentgrads.client.Client at 0x7f661ea59450>,\n",
       " <fedml_api.standalone.sailentgrads.client.Client at 0x7f661ea5b640>,\n",
       " <fedml_api.standalone.sailentgrads.client.Client at 0x7f661ea58910>,\n",
       " <fedml_api.standalone.sailentgrads.client.Client at 0x7f661ea5ae60>,\n",
       " <fedml_api.standalone.sailentgrads.client.Client at 0x7f661ea59360>,\n",
       " <fedml_api.standalone.sailentgrads.client.Client at 0x7f661ea5a0e0>]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "signal",
   "language": "python",
   "name": "signal"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
