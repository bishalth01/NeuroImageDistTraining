{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/users/rohib/miniconda3/envs/signal/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import logging\n",
    "import os\n",
    "import random\n",
    "import sys\n",
    "import pdb\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "#from fedml_api.model.cv.lenet5 import LeNet5\n",
    "# sys.path.append('/data/users2/bthapaliya/DistributedFLExperiments/DistributedFL')\n",
    "# sys.path.append('/data/users2/bthapaliya/DistributedFLExperiments/DistributedFL/fedml_api')\n",
    "\n",
    "sys.path.insert(0, os.path.abspath(\"/data/users2/bthapaliya/DistributedFLExperiments/DistributedFL/data/\"))\n",
    "from fedml_api.model.cv.salient_models import AlexNet3D_Dropout, ResNet_l3\n",
    "\n",
    "from fedml_api.data_preprocessing.cifar100.data_loader import load_partition_data_cifar100\n",
    "from fedml_api.model.cv.vgg import vgg16, vgg11\n",
    "from fedml_api.model.cv.cnn_cifar10 import cnn_cifar10, cnn_cifar100\n",
    "from fedml_api.standalone.DisPFL.dispfl_api import dispflAPI\n",
    "from fedml_api.standalone.sailentgrads.sailentgrads_api import SailentGradsAPI\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fedml_api.data_preprocessing.cifar10.data_loader import load_partition_data_cifar10\n",
    "from fedml_api.data_preprocessing.ABCD.data_loader import load_partition_data_abcd_rescale\n",
    "from fedml_api.data_preprocessing.tiny_imagenet.data_loader import load_partition_data_tiny\n",
    "from fedml_api.model.cv.resnet import  customized_resnet18, original_resnet18, tiny_resnet18\n",
    "from fedml_api.standalone.sailentgrads.my_model_trainer import MyModelTrainer\n",
    "\n",
    "from fedml_api.standalone.sailentgrads.client import Client\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Args:\n",
    "    model ='3DCNN'\n",
    "    dataset = 'ABCD'\n",
    "    data_dir ='/data/users2/bthapaliya/NeuroimageDistributedFL/SailentWeightsDistributedFL/final_dataset_1000subs.h5'\n",
    "    partition_method='dir'\n",
    "    partition_alpha=0.3\n",
    "    batch_size=16\n",
    "    client_optimizer='sgd'\n",
    "    lr=0.001\n",
    "    lr_decay=0.998\n",
    "    wd=5e-4\n",
    "    epochs=2\n",
    "    client_num_in_total = 6\n",
    "    frac = 0.5\n",
    "    momentum=0\n",
    "    comm_round=200\n",
    "    frequency_of_the_test=1\n",
    "    gpu=0\n",
    "    ci=0\n",
    "    dense_ratio=0.5\n",
    "    anneal_factor=0.5\n",
    "    seed=1024\n",
    "    cs='v0'\n",
    "    itersnip_iteration = 1\n",
    "    stratified_sampling ='store_true'\n",
    "    active=1.0\n",
    "    public_portion=0\n",
    "    erk_power_scale=1\n",
    "    dis_gradient_check=False\n",
    "    strict_avg= False\n",
    "    static = False\n",
    "    uniform = False\n",
    "    save_masks = False\n",
    "    different_initial = False\n",
    "    record_mask_diff = False\n",
    "    diff_spa = False\n",
    "    global_test = False\n",
    "    tag=\"test\"\n",
    "    snip_mask=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# python main_sailentgrads.py --model 'resnet18' \\\n",
    "# --dataset 'cifar10' \\\n",
    "# --partition_method 'dir' \\\n",
    "# --partition_alpha 0.3 \\\n",
    "# --batch_size 16 \\\n",
    "# --lr 0.1 \\\n",
    "# --lr_decay 0.998 \\\n",
    "# --epochs 5 \\\n",
    "# --dense_ratio 0.1 \\\n",
    "# --client_num_in_total 100 --frac 0.1 \\\n",
    "# --comm_round 500 \\\n",
    "# --seed 2022"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = Args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(args, model_name,class_num):\n",
    "    model = None\n",
    "    if model_name == \"3DCNN\":\n",
    "        model = AlexNet3D_Dropout(num_classes=class_num)\n",
    "    if model_name == \"cnn_cifar10\":\n",
    "        model = cnn_cifar10()\n",
    "    elif model_name == \"cnn_cifar100\":\n",
    "        model = cnn_cifar100()\n",
    "    elif model_name == \"resnet18\" and args.dataset != 'tiny':\n",
    "        model = customized_resnet18(class_num=class_num)\n",
    "    elif model_name == \"resnet18\" and args.dataset == 'tiny':\n",
    "        model = tiny_resnet18(class_num=class_num)\n",
    "    elif model_name == \"vgg11\":\n",
    "        model = vgg11(class_num)\n",
    "    return model\n",
    "\n",
    "\n",
    "def custom_model_trainer(args, model, logger):\n",
    "    return MyModelTrainer(model, args, logger)\n",
    "\n",
    "def logger_config(log_path, logging_name):\n",
    "    logger = logging.getLogger(logging_name)\n",
    "    logger.setLevel(level=logging.DEBUG)\n",
    "    handler = logging.FileHandler(log_path, mode='w+',encoding='UTF-8')\n",
    "    handler.setLevel(level=logging.DEBUG)\n",
    "    formatter = logging.Formatter('%(message)s')\n",
    "    handler.setFormatter(formatter)\n",
    "    logger.addHandler(handler)\n",
    "    return logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(args, dataset_name):\n",
    "    if dataset_name == \"ABCD\":\n",
    "        #args.data_dir += \"ABCD\"\n",
    "        train_data_num, test_data_num, train_data_global, test_data_global, \\\n",
    "        train_data_local_num_dict, train_data_local_dict, test_data_local_dict, \\\n",
    "        class_num = load_partition_data_abcd_rescale(args.data_dir, args.partition_method,\n",
    "                                args.partition_alpha, args.client_num_in_total, args.batch_size, logger)\n",
    "\n",
    "    if dataset_name == \"cifar10\":\n",
    "        args.data_dir += \"cifar10\"\n",
    "        train_data_num, test_data_num, train_data_global, test_data_global, \\\n",
    "        train_data_local_num_dict, train_data_local_dict, test_data_local_dict, \\\n",
    "        class_num = load_partition_data_cifar10(args.data_dir, args.partition_method,\n",
    "                                args.partition_alpha, args.client_num_in_total, args.batch_size, logger)\n",
    "    elif dataset_name == \"cifar100\":\n",
    "        args.data_dir += \"cifar100\"\n",
    "        train_data_num, test_data_num, train_data_global, test_data_global, \\\n",
    "        train_data_local_num_dict, train_data_local_dict, test_data_local_dict, \\\n",
    "        class_num = load_partition_data_cifar100(args.data_dir, args.partition_method,\n",
    "                                                args.partition_alpha, args.client_num_in_total, args.batch_size, logger)\n",
    "    elif dataset_name == \"tiny\":\n",
    "        args.data_dir += \"tiny_imagenet\"\n",
    "        train_data_num, test_data_num, train_data_global, test_data_global, \\\n",
    "        train_data_local_num_dict, train_data_local_dict, test_data_local_dict, \\\n",
    "        class_num = load_partition_data_tiny(args.data_dir, args.partition_method,\n",
    "                                             args.partition_alpha, args.client_num_in_total,\n",
    "                                                 args.batch_size, logger)\n",
    "\n",
    "    dataset = [train_data_num, test_data_num, train_data_global, test_data_global,\n",
    "               train_data_local_num_dict, train_data_local_dict, test_data_local_dict, class_num]\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch version1.12.1+cu116\n"
     ]
    }
   ],
   "source": [
    "print(\"torch version{}\".format(torch.__version__))\n",
    "device = torch.device(\"cuda:\" + str(args.gpu) if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "data_partition=args.partition_method\n",
    "if data_partition!=\"homo\":\n",
    "    data_partition+=str(args.partition_alpha)\n",
    "args.identity = \"SailentGrads\" + \"-\" + args.dataset + \"-\" + data_partition\n",
    "args.identity+=\"-mdl\" + args.model + \"customized\" +\"lowbatch\"\n",
    "args.identity+=\"-cs\"+args.cs\n",
    "\n",
    "args.identity += \"-cm\" + str(args.comm_round) + \"-total_clnt\" + str(args.client_num_in_total)\n",
    "args. client_num_per_round = int(args.client_num_in_total* args.frac)\n",
    "args.identity += \"-neighbor\" + str(args.client_num_per_round)\n",
    "args.identity += \"-dr\" + str(args.dense_ratio)\n",
    "args.identity += \"-active\" + str(args.active)\n",
    "args.identity += '-seed' + str(args.seed)\n",
    "args.identity += '-batchsize' + str(args.batch_size)\n",
    "args.identity += '-iteration' + str(args.itersnip_iteration)\n",
    "args.identity += '-stratified' + str(args.stratified_sampling)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_path = os.path.join('./', 'LOG/test' + '.log')\n",
    "main_log_path = os.path.join('LOG/' + args.dataset)\n",
    "if not os.path.exists(main_log_path):\n",
    "    os.makedirs(main_log_path)\n",
    "logger = logger_config(log_path='LOG/' + args.dataset + '.log', logging_name=args.identity)\n",
    "\n",
    "logger.info(args)\n",
    "logger.info(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading complete for client: 0\n",
      "Loading complete for client: 1\n",
      "Loading complete for client: 2\n",
      "Loading complete for client: 3\n",
      "Loading complete for client: 4\n",
      "Loading complete for client: 5\n"
     ]
    }
   ],
   "source": [
    "dataset = load_data(args, \"ABCD\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('abcd.pickle', 'wb') as handle:\n",
    "    pickle.dump(dataset, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('abcd.pickle', 'rb') as handle:\n",
    "    dataset = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create model.\n",
    "model = create_model(args, model_name=args.model,class_num=1)\n",
    "model = model.to(device)\n",
    "model_trainer = custom_model_trainer(args, model, logger)\n",
    "logger.info(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_clients(train_data_local_num_dict, train_data_local_dict, test_data_local_dict, model_trainer):\n",
    "    for client_idx in range(args.client_num_in_total):\n",
    "        c = Client(client_idx, train_data_local_dict[client_idx], test_data_local_dict[client_idx],\n",
    "                    train_data_local_num_dict[client_idx], args, device, model_trainer, logger)\n",
    "        client_list.append(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "[train_data_num, test_data_num, train_data_global, test_data_global,\n",
    "  train_data_local_num_dict, train_data_local_dict, test_data_local_dict, class_counts] = dataset\n",
    "\n",
    "client_list = []\n",
    "setup_clients(train_data_local_num_dict, train_data_local_dict, test_data_local_dict, model_trainer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get client model and client data\n",
    "clnt = client_list[1]\n",
    "clnt_data = clnt.local_training_data\n",
    "clnt_testdata = clnt.local_test_data\n",
    "model = clnt.model_trainer.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "x,labels,z = next(iter(clnt_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = x.to(device)  # Convert to tensor\n",
    "x = x.unsqueeze(1)\n",
    "\n",
    "x, labels = x.to(device), labels.to(device)\n",
    "model.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "args.epochs=40"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.SGD(filter(lambda p: p.requires_grad, model.parameters()), lr=0.01, momentum=args.momentum,weight_decay=args.wd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, test_data, device):\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    metrics = {\n",
    "        'test_correct': 0,\n",
    "        'test_acc':0.0,\n",
    "        'test_loss': 0,\n",
    "        'test_total': 0\n",
    "    }\n",
    "\n",
    "    criterion = nn.BCEWithLogitsLoss().to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        #for batch_idx, (x, target) in enumerate(test_data):\n",
    "        for x, target, _ in test_data:\n",
    "            #For 3DConv Network\n",
    "            #x = torch.tensor(x, dtype=torch.float32)  # Convert to tensor\n",
    "            x = x.to(device)  # Convert to tensor\n",
    "            x = x.unsqueeze(1)\n",
    "\n",
    "            #x = x.to(device)\n",
    "            target = target.to(device)\n",
    "            pred = model(x)\n",
    "            loss = criterion(pred, target.unsqueeze(1).float())\n",
    "\n",
    "            _, predicted = torch.max(pred, -1)\n",
    "            correct = predicted.eq(target).sum()\n",
    "\n",
    "            metrics['test_correct'] += correct.item()\n",
    "            metrics['test_loss'] += loss.item() * target.size(0)\n",
    "            metrics['test_total'] += target.size(0)\n",
    "            metrics['test_acc'] = metrics['test_correct'] / metrics['test_total']\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, optimizer, train_data, clnt_testdata, device,  args, round, epochs):\n",
    "    loss_acc_d = default_dict(list)\n",
    "    # torch.manual_seed(0)\n",
    "    model = model\n",
    "    model.to(device)\n",
    "    model.train()\n",
    "    # train and update\n",
    "    criterion = nn.BCEWithLogitsLoss().to(device)\n",
    "    for epoch in range(epochs):\n",
    "        epoch_loss = []\n",
    "        #for batch_idx, (x, labels) in enumerate(train_data):\n",
    "        for x, labels, _ in train_data:\n",
    "            #For 3DConv Network\n",
    "            #x = torch.tensor(x, dtype=torch.float32)  # Convert to tensor\n",
    "            x = x.to(device)  # Convert to tensor\n",
    "            x = x.unsqueeze(1)\n",
    "\n",
    "            x, labels = x.to(device), labels.to(device)\n",
    "            model.zero_grad()\n",
    "            log_probs = model.forward(x)\n",
    "            loss = criterion(log_probs, labels.unsqueeze(1).float())\n",
    "            loss.backward()\n",
    "            # to avoid nan loss\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 10)\n",
    "            optimizer.step()\n",
    "            epoch_loss.append(loss.item())\n",
    "            # for name, param in model.named_parameters():\n",
    "            #     if name in masks:\n",
    "            #         param.data *= masks[name].to(device)\n",
    "        out_d = test(model, clnt_testdata, device)\n",
    "        \n",
    "        loss_acc_d['loss'].append(sum(epoch_loss) / len(epoch_loss))\n",
    "        loss_acc_d['test_acc'].append(out_d['test_acc']) \n",
    "        print('Epoch: {}\\tLoss: {:.6f}\\tTestAcc: {:.3f}\\tLR: {:.4f}'.format(epoch, sum(epoch_loss) / len(epoch_loss), out_d['test_acc'], optimizer.param_groups[0]['lr']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0\tLoss: 0.727387\t LR: 0.0100\n",
      "Epoch: 1\tLoss: 0.726938\t LR: 0.0100\n",
      "Epoch: 2\tLoss: 0.673624\t LR: 0.0100\n",
      "Epoch: 3\tLoss: 0.617344\t LR: 0.0100\n",
      "Epoch: 4\tLoss: 0.545941\t LR: 0.0100\n",
      "Epoch: 5\tLoss: 0.665138\t LR: 0.0100\n",
      "Epoch: 6\tLoss: 0.549397\t LR: 0.0100\n",
      "Epoch: 7\tLoss: 0.601502\t LR: 0.0100\n",
      "Epoch: 8\tLoss: 0.518119\t LR: 0.0100\n",
      "Epoch: 9\tLoss: 0.575951\t LR: 0.0100\n",
      "Epoch: 10\tLoss: 0.527502\t LR: 0.0100\n",
      "Epoch: 11\tLoss: 0.606163\t LR: 0.0100\n",
      "Epoch: 12\tLoss: 0.541124\t LR: 0.0100\n",
      "Epoch: 13\tLoss: 0.494249\t LR: 0.0100\n",
      "Epoch: 14\tLoss: 0.527716\t LR: 0.0100\n",
      "Epoch: 15\tLoss: 0.569110\t LR: 0.0100\n",
      "Epoch: 16\tLoss: 0.493572\t LR: 0.0100\n",
      "Epoch: 17\tLoss: 0.555566\t LR: 0.0100\n",
      "Epoch: 18\tLoss: 0.531039\t LR: 0.0100\n",
      "Epoch: 19\tLoss: 0.483292\t LR: 0.0100\n",
      "Epoch: 20\tLoss: 0.429475\t LR: 0.0100\n",
      "Epoch: 21\tLoss: 0.470610\t LR: 0.0100\n",
      "Epoch: 22\tLoss: 0.454285\t LR: 0.0100\n",
      "Epoch: 23\tLoss: 0.425900\t LR: 0.0100\n",
      "Epoch: 24\tLoss: 0.444572\t LR: 0.0100\n",
      "Epoch: 25\tLoss: 0.474033\t LR: 0.0100\n",
      "Epoch: 26\tLoss: 0.417863\t LR: 0.0100\n",
      "Epoch: 27\tLoss: 0.485604\t LR: 0.0100\n",
      "Epoch: 28\tLoss: 0.416222\t LR: 0.0100\n",
      "Epoch: 29\tLoss: 0.465376\t LR: 0.0100\n",
      "Epoch: 30\tLoss: 0.446949\t LR: 0.0100\n",
      "Epoch: 31\tLoss: 0.356150\t LR: 0.0100\n",
      "Epoch: 32\tLoss: 0.409205\t LR: 0.0100\n",
      "Epoch: 33\tLoss: 0.373216\t LR: 0.0100\n",
      "Epoch: 34\tLoss: 0.378414\t LR: 0.0100\n",
      "Epoch: 35\tLoss: 0.301598\t LR: 0.0100\n",
      "Epoch: 36\tLoss: 0.327404\t LR: 0.0100\n",
      "Epoch: 37\tLoss: 0.235202\t LR: 0.0100\n",
      "Epoch: 38\tLoss: 0.332511\t LR: 0.0100\n",
      "Epoch: 39\tLoss: 0.163035\t LR: 0.0100\n",
      "Epoch: 40\tLoss: 0.197945\t LR: 0.0100\n",
      "Epoch: 41\tLoss: 0.293891\t LR: 0.0100\n",
      "Epoch: 42\tLoss: 0.285871\t LR: 0.0100\n",
      "Epoch: 43\tLoss: 0.203225\t LR: 0.0100\n",
      "Epoch: 44\tLoss: 0.236610\t LR: 0.0100\n",
      "Epoch: 45\tLoss: 0.252497\t LR: 0.0100\n",
      "Epoch: 46\tLoss: 0.165994\t LR: 0.0100\n",
      "Epoch: 47\tLoss: 0.157094\t LR: 0.0100\n",
      "Epoch: 48\tLoss: 0.131653\t LR: 0.0100\n",
      "Epoch: 49\tLoss: 0.299032\t LR: 0.0100\n",
      "Epoch: 50\tLoss: 0.157135\t LR: 0.0100\n",
      "Epoch: 51\tLoss: 0.123866\t LR: 0.0100\n",
      "Epoch: 52\tLoss: 0.170660\t LR: 0.0100\n",
      "Epoch: 53\tLoss: 0.133995\t LR: 0.0100\n",
      "Epoch: 54\tLoss: 0.101932\t LR: 0.0100\n",
      "Epoch: 55\tLoss: 0.049303\t LR: 0.0100\n",
      "Epoch: 56\tLoss: 0.009176\t LR: 0.0100\n",
      "Epoch: 57\tLoss: 0.120146\t LR: 0.0100\n",
      "Epoch: 58\tLoss: 0.112645\t LR: 0.0100\n",
      "Epoch: 59\tLoss: 0.010453\t LR: 0.0100\n",
      "Epoch: 60\tLoss: 0.263852\t LR: 0.0100\n",
      "Epoch: 61\tLoss: 0.090877\t LR: 0.0100\n",
      "Epoch: 62\tLoss: 0.012015\t LR: 0.0100\n",
      "Epoch: 63\tLoss: 0.007542\t LR: 0.0100\n",
      "Epoch: 64\tLoss: 0.130159\t LR: 0.0100\n",
      "Epoch: 65\tLoss: 0.013672\t LR: 0.0100\n",
      "Epoch: 66\tLoss: 0.009935\t LR: 0.0100\n",
      "Epoch: 67\tLoss: 0.088648\t LR: 0.0100\n",
      "Epoch: 68\tLoss: 0.039950\t LR: 0.0100\n",
      "Epoch: 69\tLoss: 0.123417\t LR: 0.0100\n",
      "Epoch: 70\tLoss: 0.034867\t LR: 0.0100\n",
      "Epoch: 71\tLoss: 0.008476\t LR: 0.0100\n",
      "Epoch: 72\tLoss: 0.005941\t LR: 0.0100\n",
      "Epoch: 73\tLoss: 0.004211\t LR: 0.0100\n",
      "Epoch: 74\tLoss: 0.003791\t LR: 0.0100\n",
      "Epoch: 75\tLoss: 0.002304\t LR: 0.0100\n",
      "Epoch: 76\tLoss: 0.002894\t LR: 0.0100\n",
      "Epoch: 77\tLoss: 0.001845\t LR: 0.0100\n",
      "Epoch: 78\tLoss: 0.004602\t LR: 0.0100\n",
      "Epoch: 79\tLoss: 0.003112\t LR: 0.0100\n",
      "Epoch: 80\tLoss: 0.004563\t LR: 0.0100\n",
      "Epoch: 81\tLoss: 0.005471\t LR: 0.0100\n",
      "Epoch: 82\tLoss: 0.210951\t LR: 0.0100\n",
      "Epoch: 83\tLoss: 0.038894\t LR: 0.0100\n",
      "Epoch: 84\tLoss: 0.016070\t LR: 0.0100\n",
      "Epoch: 85\tLoss: 0.003184\t LR: 0.0100\n",
      "Epoch: 86\tLoss: 0.001925\t LR: 0.0100\n",
      "Epoch: 87\tLoss: 0.003839\t LR: 0.0100\n",
      "Epoch: 88\tLoss: 0.001991\t LR: 0.0100\n",
      "Epoch: 89\tLoss: 0.001436\t LR: 0.0100\n",
      "Epoch: 90\tLoss: 0.033210\t LR: 0.0100\n",
      "Epoch: 91\tLoss: 0.004552\t LR: 0.0100\n",
      "Epoch: 92\tLoss: 0.004244\t LR: 0.0100\n",
      "Epoch: 93\tLoss: 0.001556\t LR: 0.0100\n",
      "Epoch: 94\tLoss: 0.001159\t LR: 0.0100\n",
      "Epoch: 95\tLoss: 0.527346\t LR: 0.0100\n",
      "Epoch: 96\tLoss: 0.100154\t LR: 0.0100\n",
      "Epoch: 97\tLoss: 0.003749\t LR: 0.0100\n",
      "Epoch: 98\tLoss: 0.002469\t LR: 0.0100\n",
      "Epoch: 99\tLoss: 0.001275\t LR: 0.0100\n"
     ]
    }
   ],
   "source": [
    "train(model, optimizer, clnt_data, clnt_testdata, device, args, round=0, epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.001"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimizer.param_groups[0]['lr'] = 0.001\n",
    "optimizer.param_groups[0]['lr']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dd = defaultdict(list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'test_correct': 14,\n",
       " 'test_acc': 0.5185185185185185,\n",
       " 'test_loss': 38.58154112100601,\n",
       " 'test_total': 27}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test(model, clnt_testdata, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 16\n",
      "1 11\n"
     ]
    }
   ],
   "source": [
    "for i, data in enumerate(clnt_testdata):\n",
    "    print(i, data[0].shape[0])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 0., 1., 0., 0., 0., 1., 1., 1., 1., 1.])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<fedml_api.standalone.sailentgrads.client.Client at 0x7f5ffee94400>,\n",
       " <fedml_api.standalone.sailentgrads.client.Client at 0x7f5ffee95e40>,\n",
       " <fedml_api.standalone.sailentgrads.client.Client at 0x7f5ffee97550>,\n",
       " <fedml_api.standalone.sailentgrads.client.Client at 0x7f5ffee96c80>,\n",
       " <fedml_api.standalone.sailentgrads.client.Client at 0x7f5ffee94df0>,\n",
       " <fedml_api.standalone.sailentgrads.client.Client at 0x7f5ffee96830>]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "signal",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
